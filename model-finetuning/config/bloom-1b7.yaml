data:
  filenames: examples/*.txt
  max_length: 2048
  random_state: 42

model:
  transformer:
    pretrained_model_name_or_path: bigscience/bloom-1b7
  lora:
    lora_dim: 4
    lora_scale: 8
    attention_layer_name: query_key_value

optim:
  optimizer:
    lr: 2e-4
    betas: [0.9, 0.999]
    eps: 1e-6
    weight_decay: 0.01
  scheduler:
    name: linear
    num_warmup_steps: 2500
    num_training_steps: 250000

train:
  batch_size: 4
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  gradient_checkpointing: true
  log_every_n_steps: 10
  save_every_n_train_steps: 2500
