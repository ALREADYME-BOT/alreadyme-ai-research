data:
  filenames: examples/*.txt
  max_length: 2048

model:
  pretrained_model_name_or_path: bigscience/bloom-1b7

optim:
  optimizer:
    lr: 5e-5
    betas: [0.9, 0.999]
    eps: 1e-6
    weight_decay: 0.01
  scheduler:
    name: linear
    num_warmup_steps: 1000
    num_training_steps: 1000000

train:
  batch_size: 1
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  gradient_checkpointing: true
  log_every_n_steps: 10
